{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOW5ZFb5GPx0Lm80lE2NYJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kadirov1194/Building_energy_forecasting/blob/main/Building_energy_consumption_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "Predicting energy budget of a building for a month\n",
        " - Weather data for 3 years\n",
        " - Energy consumption record of 3 years\n",
        " - Energy cost of a month"
      ],
      "metadata": {
        "id": "CJfeiHTT6Q8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use the pandas library for data analysis and manipulation.\n",
        "import pandas as pd\n",
        "\n",
        "# Often we need some functions from numpy for adding support for large, multi-dimensional arrays and matrices.\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization, we import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "     "
      ],
      "metadata": {
        "id": "vjB3-R3q7B8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the building electricity consumption data"
      ],
      "metadata": {
        "id": "TZ0VXC617ORA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # Mount to Google Colab/ MyDrive/ Colab\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SxSasTZ88Q3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Energy1 = pd.read_excel('/content/drive/MyDrive/demand_response/data/Building energy consumption racord.xlsx')\n",
        "Energy1"
      ],
      "metadata": {
        "id": "rfpY5ZMg7M57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set time column as index\n",
        "Energy = Energy1.set_index('Time')\n",
        "Energy"
      ],
      "metadata": {
        "id": "r5d8PEwS7orU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the description of the data\n",
        "Energy.describe()"
      ],
      "metadata": {
        "id": "3kzfP6Gl9Hw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(Energy['building 41']) # Energy.loc[:,'building 12'] or Energy.iloc[:,0]\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0V-zOSIh9bed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a visual representation of the data description\n",
        "import seaborn as sns\n",
        "sns.boxplot(Energy['building 41'])"
      ],
      "metadata": {
        "id": "__A0fs3h939E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weather data from the excel fiie\n",
        "Path = \"/content/drive/MyDrive/demand_response/data/WeatherData.xlsx\"\n",
        "knmi = pd.read_excel(Path)\n",
        "knmi"
      ],
      "metadata": {
        "id": "FzOPSdTd-nQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the Time column as index\n",
        "knmi = knmi.set_index(\"Time\")\n",
        "knmi"
      ],
      "metadata": {
        "id": "oJutHTgH_XnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenating the datasets of weather data and electricity consumption\n",
        "df = pd.concat([knmi, Energy], axis=1) #axis =1 for considering the columns\n",
        "df"
      ],
      "metadata": {
        "id": "-LAwEP9bIUc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing data status\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "_qCXH_ax_8eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross check the availability of the data with missingno library\n",
        "import missingno as msno\n",
        "msno.bar(df)"
      ],
      "metadata": {
        "id": "TpfzCYzOASTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have no missing values in the dataframe. Our output (dependent variable) is building 12 column. other columns of the data set are independent values. We can find strong and weak correlation with different variables. With .corr() method, we can utilize Pearson's correlation coefficient which is a measure of the strength of a linear association between two variables."
      ],
      "metadata": {
        "id": "GJJNKlOUAyFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (16,6)) # Create matplotlib figure\n",
        "sns.heatmap(df.corr(), annot = True, linewidths=1, fmt=\".2g\", cmap= 'coolwarm') \n",
        "# fmt = .1e (scientific notation), .2f (2 decimal places), .3g(3 significant figures), .2%(percentage with 2 decimal places)\n",
        "plt.xticks(rotation='horizontal')"
      ],
      "metadata": {
        "id": "Nzvv8-6BAsMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap, we see temperature (Temp) correlates very positively with building electricity demand. Relative humidity (U) and hourly sum of precipitation (RH) are two highest negatively correlated features. in addition, both of these features are also multi-collinear. Which means, either of them can be utilized for predicting electricity demand.\n"
      ],
      "metadata": {
        "id": "_kiSvgZGBjyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot energy consumption data againts U and Temp"
      ],
      "metadata": {
        "id": "gHxqY7nBBpOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample the energy of the building over a week using the resmaple function and the mean  function. \n",
        "df_sum_weekly = df['building 41'].resample('W').mean()\n",
        "\n",
        "# Resample the temperature over a week. \n",
        "df_feature1= df[\"Temp\"].resample(\"W\").mean()\n",
        "\n",
        "# Resample the relative humidity over a week. \n",
        "df_feature2 = df[\"U\"].resample(\"W\").mean()"
      ],
      "metadata": {
        "id": "8_uOcwcvA5HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the result\n",
        "fig,ax = plt.subplots(figsize=(24,8))  # Create matplotlib figure\n",
        "ax.plot(df_sum_weekly.index, df_sum_weekly, color=\"red\",marker=\"o\")\n",
        "ax.set_ylabel(\"KWh\")\n",
        "ax.set_xlabel('Date')\n",
        "ax2 = ax.twinx() #Create a new Axes with an invisible x-axis and an independent y-axis positioned opposite to the original one (i.e. at right).\n",
        "ax3 = ax.twinx()\n",
        "ax2.plot(df_sum_weekly.index, df_feature1, color=\"blue\", marker=\"o\")\n",
        "ax2.set_ylabel(\"°C\")\n",
        "ax3.plot(df_sum_weekly.index, df_feature2, color=\"green\", marker=\"o\")\n",
        "ax3.set_ylabel(\"grams/cubic meter\")\n",
        "ax3.spines[\"right\"].set_position((\"axes\", 1.05))\n",
        "fig.legend([\"Resampled Weekly Energy Consumption\",\"Resampled Weekly Temperature\",\"Resampled Weekly Relative Humidity\"], loc='upper right')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "9WD3LPW0B02v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that energy demand of a building varies with temperature. Variations of the energy consumption across various seasons are also visible. Negative linear correlation of Relative Humidity can be explained. It is not just correlational with Energy consumption but also has high negative correlation (-0.57) with temperature. The correlations observed are well expected."
      ],
      "metadata": {
        "id": "U06roZ0FKwW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection\n",
        "We can now select features based on their strong coorealtion with the output and remove some input features which are strongly coorelated with each other to avoid the problem of multicolinearity. It is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\n",
        "# Exploring non-linear correlation between Energy with Hour and Month\n",
        "We use spearman's correlation between two variables. The Spearman rank-order correlation coefficient is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. This one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases."
      ],
      "metadata": {
        "id": "6Vo6Bo3wLQ2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the spearmans's correlation between two variables\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "#filter columns from dataframe\n",
        "energy = np.array(df[\"building 41\"]) \n",
        "hour = np.array(df[\"HH\"])\n",
        "month= np.array(df[\"month\"])\n",
        "\n",
        "# calculate spearman's correlation\n",
        "corr1, _ = spearmanr(energy, hour)\n",
        "corr2,_ = spearmanr(energy, month)\n",
        "print('Spearmans correlation between Energy and hour feature: %.3f' % corr1)\n",
        "print('Spearmans correlation between Energy and month feature: %.3f' % corr2)"
      ],
      "metadata": {
        "id": "EQjSKvyCIg8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see, the energy consumption has a seasonal effet which is reflected on the different months of the year. So it has more correlation with month than hours of the day."
      ],
      "metadata": {
        "id": "RCL1qWHJLtH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce number of features with lower correlation values or it has an inverse effect on the results of the model.\n",
        "knmi_updated= knmi.loc[:, ~knmi.columns.isin([\"TD\",\"U\",\"DR\",\"FX\"])] # ~ sign drops the columns we select\n",
        "knmi_updated"
      ],
      "metadata": {
        "id": "SmWVh-MkLjw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we develop a machine learning regression model based on the weather parameters to predict the energy consumption of the building.\n",
        "Various forecasting techniques can be utilized with machine learning models. (Deng et al., 2018) tested the performance of various machine learning models on one of the largest database on buildings in CBECS, and found both Support Vector Machine (SVM) and Random Forest (RF) being able to handle the non-linear relationships better as they perform dynamic local investigations better rather than global optimization. Therefore, we are going to consider SVM and RF to develop the model."
      ],
      "metadata": {
        "id": "4f6PGS_eMaA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data into training (80%) and testing (20%) set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(knmi_updated, Energy, test_size = 0.2, random_state = 0)\n",
        "y_train"
      ],
      "metadata": {
        "id": "EC5RnyigML2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.values.ravel() #ravel is a numpy function to change a 2-dimensional array or a multi-dimensional array into a continuous flattened array.\n",
        "y_train"
      ],
      "metadata": {
        "id": "6_GiA2mKMjSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test.values.ravel()\n",
        "y_test"
      ],
      "metadata": {
        "id": "ytKHiMobNHcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing regression model \n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#Creating an instance or object of the support vector machine regressor class\n",
        "SVReg = SVR(kernel= 'rbf') # It must be one of 'linear', 'poly', 'rbf', 'sigmoid' (rbf - Radial Basis Function is used in machine learning to find a non-linear regression line.)\n",
        "\n",
        "# fitting the regression model to the training dataset\n",
        "SVReg.fit(X_train, y_train) #Fit the SVM model according to the given training data."
      ],
      "metadata": {
        "id": "piMZxr-UNKEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three hyperparameters for the SVR function are Epsilon, C and Gamma.\n",
        "\n",
        "Epsilon represents the maximum error allowed in the function, and determines the desired accuracy of the model. A smaller ϵ indicates that a more accurate model is required.\n",
        "\n",
        "Parameters C is used to set the tolerance for points which fall outside of the error boundaries set by ϵ. A larger value for C indicates a larger tolerance for points outside of ϵ.\n",
        "\n",
        "The gamma parameter defines the influence of a single point on the model. A low value indicates that points have a far reach, meaning that also points that are situated far away from the regression line are taken into account. A low value results in a more linear regression line. On the other hand, a high value for gamma indicates that points have a close reach, which results in a more complex or wobbly line around the nearby points. With default 'scale', the value of gamma is 1 / (n_features * X.var())."
      ],
      "metadata": {
        "id": "j7h0vTh0OHHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on the training data\n",
        "Predicted_Train= SVReg.predict(X_train)\n",
        "Predicted_Train"
      ],
      "metadata": {
        "id": "tj0TB93BNzhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To evaluate the performance of the model, importing error metrics function\n",
        "from sklearn.metrics import r2_score #(coefficient of determination) regression score function.\n",
        "from sklearn.metrics import mean_squared_error #The MSE indicates the average distance of the best fit regression line to the observed values.\n",
        "\n",
        "print(r2_score(y_train,Predicted_Train))\n",
        "print(mean_squared_error(y_train,Predicted_Train))"
      ],
      "metadata": {
        "id": "Jd3TTGw2ONpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling to improve the model performance\n",
        "Scaling is used to bring all features to the same level of magnitudes. Without scaling, the features with high magnitudes will have more weight in the ‘best fit’ calculation, which tries to minimize the distance between the fit line and the observed values (Asaithambi, 2017)."
      ],
      "metadata": {
        "id": "Tk7gndBGOpc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required packages\n",
        "from sklearn.preprocessing import StandardScaler #standardizes the data to a range in which the mean is equal to 0 and the standard deviation is 1. It assumes the data is normally distributed.\n",
        "from sklearn.preprocessing import MinMaxScaler #normalizes the data and brings the values between 0 (lowest value) and 1 (highest value)\n",
        "from sklearn.preprocessing import RobustScaler #standardizes the data. But is more robust to outliers because it only scales the data according to the Interquartile Range (IQR) between the 1st and 3rd quartile.\n",
        "\n",
        "#Generate the scaler\n",
        "sc1= StandardScaler()\n",
        "sc2= MinMaxScaler()\n",
        "sc3= RobustScaler()"
      ],
      "metadata": {
        "id": "36Jc40ktObv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the input data\n",
        "X1 = sc1.fit_transform(knmi_updated)\n",
        "X2 = sc2.fit_transform(knmi_updated)\n",
        "X3 = sc3.fit_transform(knmi_updated)\n",
        "\n",
        "#We do not need to scale the output data as we have only one output."
      ],
      "metadata": {
        "id": "_RjY35z4Ougu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting to visually explore the scaled features\n",
        "sns.distplot(X1,color=\"red\",label=\"StdScaler\")\n",
        "sns.distplot(X2,color=\"blue\",label=\"MinMax\")\n",
        "sns.distplot(X3,color=\"black\",label=\"RobustScaler\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "_3bStySnOxie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split your data set into training (80%) and test data (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle= \"False\")\n",
        "y_train = y_train.values.ravel()\n",
        "y_test = y_test.values.ravel()"
      ],
      "metadata": {
        "id": "R9X55JjtPw8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the regressor and fit the training data to the regressor\n",
        "regr = SVR(kernel='rbf')\n",
        "regr= regr.fit(X_train, y_train)\n",
        "regr"
      ],
      "metadata": {
        "id": "Ky6j_Rd5QNyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the regression model to the training data\n",
        "regr.fit(X_train, y_train) #Fit the SVM model according to the given training data.\n",
        "# predicting on the training data\n",
        "predict_train= regr.predict(X_train)"
      ],
      "metadata": {
        "id": "MagZgFXaQfkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the model training accuracy \n",
        "print(r2_score(y_train, predict_train))\n",
        "print(mean_squared_error(y_train, predict_train))"
      ],
      "metadata": {
        "id": "rVj2bGgRQ0WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting on the test data\n",
        "pred= regr.predict(X_test)\n",
        "##testing the models accuracy on the test data\n",
        "print(r2_score(y_test, pred))\n",
        "print(mean_squared_error(y_test, pred))"
      ],
      "metadata": {
        "id": "Ea5es-NYRD7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we test for MnMax scaling"
      ],
      "metadata": {
        "id": "F6EyzDyyRLbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split your data set into training (80%) and test data (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, Energy, test_size=0.2, random_state=0, shuffle= \"False\")\n",
        "y_train = y_train.values.ravel()\n",
        "y_test = y_test.values.ravel()\n",
        "\n",
        "#building the regressor and fit the training data to the regressor\n",
        "regr = SVR(kernel='rbf')\n",
        "regr= regr.fit(X_train, y_train)\n",
        "\n",
        "# fitting the regression model to the training data\n",
        "regr.fit(X_train, y_train) #Fit the SVM model according to the given training data.\n",
        "# predicting on the training data\n",
        "predict_train= regr.predict(X_train)\n",
        "\n",
        "#testing the model training accuracy \n",
        "print(r2_score(y_train, predict_train))\n",
        "print(mean_squared_error(y_train, predict_train))"
      ],
      "metadata": {
        "id": "eHTFKss0RG0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting on the test data\n",
        "pred= regr.predict(X_test)\n",
        "##testing the models accuracy on the test data\n",
        "print(r2_score(y_test, pred))\n",
        "print(mean_squared_error(y_test, pred))"
      ],
      "metadata": {
        "id": "9J9EJx0cRTLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finnaly, we test for Robust scaling"
      ],
      "metadata": {
        "id": "Ru-0POliR4u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split your data set into training (80%) and test data (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X3, Energy, test_size=0.2, random_state=0, shuffle= \"False\")\n",
        "y_train = y_train.values.ravel()\n",
        "y_test = y_test.values.ravel()\n",
        "\n",
        "#building the regressor and fit the training data to the regressor\n",
        "regr = SVR(kernel='rbf')\n",
        "regr= regr.fit(X_train, y_train)\n",
        "\n",
        "# fitting the regression model to the training data\n",
        "regr.fit(X_train, y_train) #Fit the SVM model according to the given training data.\n",
        "# predicting on the training data\n",
        "predict_train= regr.predict(X_train)\n",
        "\n",
        "#testing the model training accuracy \n",
        "print(r2_score(y_train, predict_train))\n",
        "print(mean_squared_error(y_train, predict_train))"
      ],
      "metadata": {
        "id": "gbp0C7tyRyBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting on the test data\n",
        "pred= regr.predict(X_test)\n",
        "##testing the models accuracy on the test data\n",
        "print(r2_score(y_test, pred))\n",
        "print(mean_squared_error(y_test, pred))"
      ],
      "metadata": {
        "id": "nYJ81uXxSOU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that, when the R2 value increases and RMS error decreases from the previous model, we get a better performing model. Therefore, Standard scaler is the best fit for our model which can explain 86.78% of the variance of the training dataset and 86.52% of the variance of the test dataset. The prediction accuracy will vary +-2.3 (root mean squared error of 5.5)."
      ],
      "metadata": {
        "id": "e5s0j0fzSeDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same way we can compare between different karels"
      ],
      "metadata": {
        "id": "JGMRT7wKShtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split your data set into training (80%) and test data (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, Energy, test_size=0.2, random_state=0, shuffle= \"False\")\n",
        "y_train = y_train.values.ravel()\n",
        "y_test = y_test.values.ravel()\n",
        "\n",
        "#building the regressor and fit the training data to the regressor\n",
        "regr = SVR(kernel='poly', degree=5) # y = ax5 + bx4 + cx3 + dx2 + ex + f\n",
        "\n",
        "# fitting the regression model to the training data\n",
        "regr.fit(X_train, y_train) #Fit the SVM model according to the given training data.\n",
        "# predicting on the training data\n",
        "predict_train= regr.predict(X_train)\n",
        "\n",
        "#testing the model training accuracy \n",
        "print(r2_score(y_train, predict_train))\n",
        "print(mean_squared_error(y_train, predict_train))"
      ],
      "metadata": {
        "id": "8u4SOSLPSf43"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}